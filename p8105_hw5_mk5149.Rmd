---
title: "p8105_hw5_mk5149"
output: github_document
date: "2025-11-05"
author: "Mungyu Kwok"
---
# Preparation
Load any necessary packages.
```{r}
library(tidyverse)
library(broom)
```

# Problem 1
We set a random seed to ensure our simulation results are reproducible. 
```{r}
set.seed(42) # Set seed for reproducible results
```

This function `bday_sim` takes a group size `n_room` as input, samples `n_room` birthdays from 1 to 365, and returns `TRUE` if a duplicate birthday is found and `FALSE` otherwise.
```{r}
bday_sim = function(n_room){
  # Sample n_room birthdays from 1:365 with replacement
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  # Check if the number of unique birthdays is less than the group size
  # If it is, a duplicate must exist.
  repeated_bday = length(unique(birthdays)) < n_room
  
  # Return the logical result
  repeated_bday
}
```

Now, we run the simulation. We need to run it 10,000 times for each group size between 2 and 50.

We use `expand_grid` to create a tibble (a data frame) with every combination of group_size (from 2 to 50) and iter (from 1 to 10,000).

We then mutate a new column `has_duplicate` by applying our `bday_sim` function to each g`roup_size` value using `map_lgl`.

Finally, we `group_by(group_size)` and summarize to calculate the probability by taking the `mean()` of the logical `has_duplicate` column (where TRUE = 1 and FALSE = 0).
```{r}
# Define simulation parameters
n_sims = 10000
group_sizes = 2:50

# Create a tibble with all combinations of group sizes and iterations
sim_grid = 
  expand_grid(
    group_size = group_sizes,
    iter = 1:n_sims
  )

# Run the simulation and calculate probabilities
bday_sim_results = 
  sim_grid |>
  mutate(
    # Apply the simulation function to each group_size value
    has_duplicate = map_lgl(group_size, bday_sim)
  ) |>
  group_by(
    group_size
  ) |>
  summarize(
    # The mean of TRUE/FALSE values gives the probability
    probability = mean(has_duplicate)
  )

# View the first few rows of the result
knitr::kable(head(bday_sim_results))
```

Finally, we create a plot of the probability as a function of group size.
```{r}
bday_sim_results |>
  ggplot(aes(x = group_size, y = probability)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Birthday Problem: Probability of a Shared Birthday",
    x = "Group Size",
    y = "Probability of at least one shared birthday",
    caption = "Based on 10,000 simulations per group size"
  ) +
  theme_minimal() +
  # Add horizontal lines for reference
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.99, linetype = "dashed", color = "blue")
```
The plot shows the probability of a shared birthday as a function of the group size. The probability starts near 0 for a small group (e.g., at `n = 2`, the probability is 1/365, or ~0.0027) and increases rapidly as the group size grows.

The plot shows:

- The probability crosses 50% (the red dashed line) with a group size of just 23 people.

- The probability is already ~97% by the time the group size reaches 50.

- The probability crosses 99% (the blue dashed line) with a group size of 57 (which is outside our simulation range but visible on the trend).


# Problem 2
We also set a random seed here to ensure our simulation results are reproducible.
```{r}
set.seed(8105) # For reproducible results
```
 
We need to run 5000 simulations for each value of $\mu$ from 0 to 6. We create a function to run a single simulation. Function `run_one_t_test` runs `t.test()`, saves the result as `test_obj`, and then manually creates a tibble by pulling out the `estimate` and `p.value` from that object.
```{r}
run_one_t_test = function(true_mu, n = 30, sigma = 5) {
  # Generate data from Normal(µ, σ)
  x = rnorm(n = n, mean = true_mu, sd = sigma)
  
  # Perform a t-test against H0: µ = 0
  # Use broom::tidy to extract results
  broom::tidy(t.test(x, mu = 0)) |> 
    select(estimate, p.value) # We only need these two columns
  
}
```

Now, we create our simulation grid. We use `expand_grid` to create a tibble with all combinations of our true_mu values ({0, 1, 2, 3, 4, 5, 6}) and an iteration number (from 1 to 5000). This gives us $7 \times 5000 = 35,000$ total simulations to run. We use `mutate` and `map` to apply our `run_one_t_test` function to each value of `true_mu`. `unnest` then neatly expands the tibbles returned by our function.
```{r}
# Define simulation parameters
n_reps = 5000
mu_values = 0:6

# Create the simulation grid
sim_grid = 
  expand_grid(
    true_mu = mu_values,
    iter = 1:n_reps
  )

# Run all simulations
# The `map` function applies our t-test function to each `true_mu`
sim_results = 
  sim_grid |>
  mutate(
    test_output = map(true_mu, run_one_t_test)
  ) |>
  unnest(test_output)

# View the resulting structure
knitr::kable(head(sim_results))
```

## Part 1: Power Plot
To analyze power, we group our results by the `true_mu` and calculate the proportion of times the null hypothesis was rejected. A rejection occurs when `p.value < 0.05`. The `mean(p.value < 0.05)` (where `TRUE=1`, `FALSE=0`) gives us this proportion, which is the empirical power.
```{r}
# Calculate power for each true_mu
power_results = 
  sim_results |>
  group_by(true_mu) |>
  summarize(
    power = mean(p.value < 0.05)
  )

# Plot power vs. true_mu
power_results |>
  ggplot(aes(x = true_mu, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Statistical Power vs. True Effect Size (µ)",
    x = "True Mean (µ)",
    y = "Power (Proportion of Rejections)",
    caption = "n=30, σ=5, α=0.05, 5000 simulations per point"
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()
```

The plot shows a strong positive association between the effect size (the true value of $\mu$) and power.

- When the true $\mu$ is 0 (meaning the null hypothesis is true), the power is approximately 5%. This is the Type I error rate ($\alpha$), which is exactly what we set our significance level to. We correctly rejected the (true) null hypothesis about 5% of the time.
- As the true $\mu$ increases, moving further away from the null value of 0, the power (the probability of correctly rejecting the false null) increases rapidly.
- By the time the true $\mu$ reaches 4, the test has nearly 100% power, meaning it almost always detects the effect.

## Part 2: Estimate Plot
For this part, we need to compare two quantities:The average estimate $\hat{\mu}$ across all 5000 tests, and the average estimate $\hat{\mu}$ across only the tests where the null was rejected (p.value < 0.05). We calculate these two values separately and then join them into one data frame for plotting.
```{r}
# 1. Average estimate across ALL simulations
avg_estimate_all = 
  sim_results |>
  group_by(true_mu) |>
  summarize(
    avg_mu_hat = mean(estimate)
  )

# 2. Average estimate ONLY in rejected (significant) simulations
avg_estimate_rejected = 
  sim_results |>
  filter(p.value < 0.05) |>
  group_by(true_mu) |>
  summarize(
    avg_mu_hat = mean(estimate)
  )

# Combine for plotting
plot_data = 
  bind_rows(
    "All simulations" = avg_estimate_all,
    "Rejected simulations only" = avg_estimate_rejected,
    .id = "sample_group"
  )

# Plot both lines
plot_data |>
  ggplot(aes(x = true_mu, y = avg_mu_hat, color = sample_group)) +
  geom_point() +
  geom_line(linewidth = 1) +
  # Add a y=x line to show what a perfect estimate would be
  geom_abline(
    intercept = 0, slope = 1, 
    linetype = "dashed", color = "black"
    ) +
  labs(
    title = "Average Estimate vs. True Mean",
    x = "True Mean (µ)",
    y = "Average Estimated Mean (µ-hat)",
    color = "Sample Group"
  ) +
  theme_minimal()
```

Is the sample average of $\hat{\mu}$ across tests for which the null is rejected approximately equal to the true value of $\mu$? Why or why not?

No, it is not, especially for small values of the true $\mu$ (where power is low).This phenomenon is a form of selection bias.

- Why it happens: The plot shows that the average $\hat{\mu}$ from all simulations (the red line) falls perfectly on the $y=x$ dashed line. This confirms that the sample mean $\hat{\mu}$ is an unbiased estimator of the true $\mu$.
- However, the blue line (Rejected simulations only) is biased high. To get a "significant" result (reject the null), the t-statistic ($t = \hat{\mu} / SE$) must be large.
- When the true effect $\mu$ is small (e.g., $\mu=1$), the power is low. The only way to get a rejection in this scenario is if, by random chance, the sample we drew produced an estimate $\hat{\mu}$ that is much larger than the true $\mu$.
- By filtering for p.value < 0.05, we are selectively filtering for these overestimates. We are throwing away all the non-significant tests where $\hat{\mu}$ was (correctly) close to the small true $\mu$.
- As the true $\mu$ increases, power approaches 100%. This means all simulations result in a rejection, so the "rejected only" group becomes the "all simulations" group. This is why the red line converges with the blue line at higher values of $\mu$.

# Problem 3
## Step 1: Load and Describe the raw data
We will read the data directly from the Washington Post's GitHub repository raw URL.
```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_df = read_csv(url)
head(homicide_df)
```
**Description of the raw data**: The raw data contains 52,179 rows and 12 columns. Each row represents a single homicide case. Important variables include the victim's demographic information (`victim_last`, `victim_first`, `victim_race`, `victim_age`, `victim_sex`), the location of the homicide (`city`, `state`, `lat`, `lon`), and the current status of the case (`disposition`).

## Step 2: Create `city_state` variable and Summarize
We need to combine the city and state columns. We also need to define what counts as "unsolved".
- Create city_state using str_c.
- Filter out clear data entry errors (e.g., "Tulsa, AL" is a known typo in this dataset representing only one entry).
- Create a summary table with total homicides and unsolved homicides.
```{r}
homicide_summary = 
  homicide_df |> 
  # Create the city_state variable
  mutate(city_state = str_c(city, ", ", state)) |> 
  # # Remove the one erroneous Tulsa, AL entry
  # filter(city_state != "Tulsa, AL") |> 
  # Group by city to calculate totals
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    # Count homicides where disposition is NOT "Closed by arrest"
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

# View the first few rows
knitr::kable(head(homicide_summary))
```

## Step 3: Analysis for Baltimore, MD
We will perform a proportion test just for Baltimore.

- Filter for Baltimore to get the numbers (Unsolved: 1825, Total: 2827).
- Run prop.test and save it as an R object.
- Use `broom::tidy` to convert the complex test result into a neat dataframe and extract the required columns.
```{r}
# 1. Get Baltimore data
baltimore_df = 
  homicide_summary |> 
  filter(city_state == "Baltimore, MD")

# 2. Run prop.test
# x is the number of successes (unsolved cases), n is the total trials (total cases)
baltimore_test = 
  prop.test(
    x = baltimore_df$unsolved_homicides, 
    n = baltimore_df$total_homicides
  )

# 3. Tidy the result and pull estimates
baltimore_results = 
  baltimore_test |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)

baltimore_results
```

# Step 4: Iteration for All Cities
Now we use `purrr::map2` to apply that same test to every city in our summary dataframe at once.

- Use `mutate` and `map2` to run `prop.test` for every row. We use `map2` because we need two inputs: `unsolved_homicides` and `total_homicides`.
- Use `map` and `broom::tidy` to clean up the results.
- `unnest` the tidied results to expand them back into standard columns.
```{r}
city_results = 
  homicide_summary |> 
  mutate(
    # Run prop.test for each city using map2
    test_results = map2(unsolved_homicides, total_homicides, \(x, n) prop.test(x, n)),
    # Tidy the results
    tidy_results = map(test_results, broom::tidy)
  ) |> 
  select(city_state, tidy_results) |> 
  # Expand the tidy results back into columns
  unnest(tidy_results) |> 
  # Select only what we need for plotting
  select(city_state, estimate, conf.low, conf.high)

head(city_results)
```

# Step 5: Plotting Estimates and CIs
Finally, we create the plot.

- Use `ggplot` with `city_state` on one axis and estimate on the other.
- Use `geom_point` for the estimated proportion.
- Use `geom_errorbar` for the confidence intervals.
- Use `fct_reorder` to sort cities by the estimate for better readability.
```{r}
city_results |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme_minimal() +
  # Flip coordinates so city names are readable on the Y axis
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides in U.S. Cities",
    x = "City",
    y = "Proportion of Unsolved Homicides (with 95% CI)",
    caption = "Data from the Washington Post"
  )
```



